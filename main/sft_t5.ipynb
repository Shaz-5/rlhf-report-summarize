{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "559dc049",
   "metadata": {},
   "source": [
    "# Supervised Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d485f869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install evaluate transformers[torch] nltk rouge_score sentencepiece tensorboard matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4656e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from transformers import (\n",
    "    T5ForConditionalGeneration,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    ")\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fad54ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed\n",
    "\n",
    "def set_seed(seed_val=42):\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "079f3654",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"../models/sft_t5\"\n",
    "\n",
    "# hyperparameters\n",
    "train_batch_size = 16\n",
    "gradient_accumulation_steps = 1\n",
    "learning_rate = 1e-4\n",
    "eval_batch_size = 1\n",
    "eval_steps = 100\n",
    "max_input_length = 512\n",
    "# save_steps = 300\n",
    "num_train_epochs = 5\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c503dd50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded T5.\n"
     ]
    }
   ],
   "source": [
    "# model - t5\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "print('Loaded T5.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c19d18d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SFTDataset(Dataset):\n",
    "    def __init__(self, data_path, tokenizer, max_length=512):\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        df = pd.read_csv(data_path)\n",
    "        \n",
    "        self.texts = df['findings'].tolist()\n",
    "        self.summaries = df['impression'].tolist()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        text = self.texts[idx]\n",
    "        summary = self.summaries[idx]\n",
    "        \n",
    "        input_text = f\"summarize: {text}\"\n",
    "        \n",
    "        input_encodings = self.tokenizer(\n",
    "            input_text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        target_encodings = self.tokenizer(\n",
    "            summary,\n",
    "            max_length=self.max_length // 2,  # summaries should be shorter\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        input_ids = input_encodings[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = input_encodings[\"attention_mask\"].squeeze(0)\n",
    "        labels = target_encodings[\"input_ids\"].squeeze(0)\n",
    "        \n",
    "        # replace padding token id with -100 for labels\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2206dcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "\n",
    "train_dataset = SFTDataset(\n",
    "    '../data/preprocessed/train.csv',\n",
    "    tokenizer,\n",
    "    max_length=max_input_length,\n",
    ")\n",
    "\n",
    "val_dataset = SFTDataset(\n",
    "    '../data/preprocessed/val.csv',\n",
    "    tokenizer,\n",
    "    max_length=max_input_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53103430",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6.27k/6.27k [00:00<00:00, 7.55MB/s]\n"
     ]
    }
   ],
   "source": [
    "# rouge metric\n",
    "rouge = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "122bd7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_metrics(eval_preds):\n",
    "#     predictions = eval_preds.predictions\n",
    "#     labels = eval_preds.label_ids\n",
    "\n",
    "#     # Decode generated summaries\n",
    "#     decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "#     print(decoded_preds)\n",
    "        \n",
    "#     # Replace -100 in the labels as we can't decode them\n",
    "#     labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "#     decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "#     result = rouge.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1162ed0c",
   "metadata": {},
   "source": [
    "## Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ab08b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "WARNING:accelerate.utils.other:Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_accumulation_steps=1,\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=train_batch_size,\n",
    "    per_device_eval_batch_size=eval_batch_size,\n",
    "    gradient_checkpointing=True,\n",
    "    half_precision_backend=True,\n",
    "    fp16=True,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.999,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    warmup_steps=100,\n",
    "    eval_steps=eval_steps,\n",
    "    save_steps=0,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=10,\n",
    "    logging_dir='../logs/sft_t5',\n",
    "    log_level='info',\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "#     compute_metrics=compute_metrics,\n",
    "    data_collator=default_data_collator,\n",
    "#     preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9cfcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    trainer.train()\n",
    "except KeyboardInterrupt:\n",
    "    print('Training Stopped.')\n",
    "    \n",
    "# trainer.save_model(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc45d2e5",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e770593",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_dataset = SFTDataset(\n",
    "    '../data/preprocessed/test.csv',\n",
    "    tokenizer,\n",
    "    max_length=max_input_length,\n",
    ")\n",
    "\n",
    "model_path = next((f\"../models/sft_t5/{folder}\" for folder in os.listdir('../models/sft_t5') if re.search(r'checkpoint-\\d+', folder)), None)\n",
    "model.from_pretrained(model_path)\n",
    "print('Loaded saved model.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b25199b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_eval_results = trainer.evaluate(test_dataset)\n",
    "\n",
    "print('Trainer evaluation results on test set: ')\n",
    "print(trainer_eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b003686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(model, test_dataset, tokenizer, batch_size=8, device='cuda'):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_references = []\n",
    "    \n",
    "    # dataloader\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                num_beams=4,\n",
    "                max_length=150,\n",
    "            )\n",
    "            \n",
    "            decoded_preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            \n",
    "            # replace -100 with pad_token_id for decoding\n",
    "            labels[labels == -100] = tokenizer.pad_token_id\n",
    "            decoded_refs = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "            \n",
    "            all_predictions.extend(decoded_preds)\n",
    "            all_references.extend(decoded_refs)\n",
    "    \n",
    "    # ROUGE scores\n",
    "    rouge = evaluate.load('rouge')\n",
    "    results = rouge.compute(predictions=all_predictions, references=all_references)\n",
    "    \n",
    "    return results, all_predictions, all_references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c723c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "# model.to('cuda')\n",
    "eval_results, preds, refs = run_evaluation(model, test_dataset, tokenizer)\n",
    "\n",
    "print(\"\\nEvaluation Results:\")\n",
    "for metric, score in eval_results.items():\n",
    "    print(f\"{metric}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0003d9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved eval results.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "os.makedirs('../results/sft_t5', exist_ok=True)\n",
    "\n",
    "# save scores\n",
    "with open(f'../results/sft_t5/rouge_scores.pkl', 'wb') as f:\n",
    "    pickle.dump(eval_results, f)\n",
    "\n",
    "# save predictions\n",
    "with open(f'../results/sft_t5/predictions.pkl', 'wb') as f:\n",
    "    pickle.dump(preds, f)\n",
    "\n",
    "# save references\n",
    "with open(f'../results/sft_t5/references.pkl', 'wb') as f:\n",
    "    pickle.dump(refs, f)\n",
    "    \n",
    "print('Saved eval results.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
